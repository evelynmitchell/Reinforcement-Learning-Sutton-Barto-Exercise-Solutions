### A Pluto.jl notebook ###
# v0.19.24

using Markdown
using InteractiveUtils

# ╔═╡ d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
using PlutoUI, PlutoPlotly, Random, Distributions, StatsBase, LinearAlgebra, LaTeXStrings, Base.Threads

# ╔═╡ 36a6e43f-6bcf-4c27-bfbb-047760e77ada
md"""
# Chapter 13 Policy Gradient Methods Introduction
Instead of selection actions based on *action-value estimates* we learn a *parameterized policy* with parameters θ.  $\pi(a|s, \mathbf{\theta}) = \text{Pr}\{A_t=a|S_t=s, \mathbf{\theta}_t=\mathbf{\theta\}}$ denotes the probability that action *a* is taken at time *t* given that the environment is in state *s* at time *t* with parameter **θ**.  

We consider methods that improve the policy parameter using the gradient of some scalar performance measure $J(\mathbf{\theta})$ with respect to the policy parameters.  We follow gradient ascent since we are trying to maximize this value and methods that use this approach are called *policy gradient methods*.  Methods that learn approximations to both policy and value functions are often called *actor-critic methods*, where 'actor' is a reference to the learned policy, and 'critic' refers to the learned value function, usually a state-value function.
# 13.1 Policy Approximation and its Advantages
"""

# ╔═╡ 2501cbc0-9772-4b2f-ab01-ef7903e62950
md"""
If the state/action space is discrete and not too large then we can have numerical preferences for each state/action pair parameterized by θ.  $h(s, a \mathbf{\theta})$ and the corresponding policy can be to select actions according to the probability distribution generated by the soft-max.  $\pi(a|s, \mathbf{\theta}) \dot = \frac{\exp{h(s, a, \mathbf{\theta})}}{\sum_b \exp{h(s, b, \mathbf{\theta})}}$.  One advantage of using the soft-max is that the optimal policy can be stochastic or we can approach a deterministic policy by selecting the action with the highest probability.  If we include a temperature parameter in the soft-max then we can vary the same policy to be more or less stochastic as needed.

Another advantage is that for some problems the policy may be easier to approximate than the action-value function.  We can also inject some prior knowledge of the environment into how the policy is parametrized.
"""

# ╔═╡ 7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
md"""
## Example 13.1 Short corridor gridworld
"""

# ╔═╡ 1b89a5be-d4f6-43b6-b778-0895d77d0962
abstract type LinearMove end

# ╔═╡ 759afa53-2b01-4d9b-b398-80120626634f
struct Left <: LinearMove end

# ╔═╡ 97046258-7753-4edb-b0c9-0981d587ad35
struct Right <: LinearMove end

# ╔═╡ 423321cc-1c8c-44a0-bd8e-a4d3cb68962b
function make_corridor()
	move(s::Integer, ::Left) = s == 2 ? s + 1 : s - 1
	move(s::Integer, ::Right) = s == 2 ? s - 1 : s + 1
	function step(s::Integer, a::LinearMove)
		s′ = max(1, move(s, a))
		(s′, -1.0)
	end
	(states = 1:3, sterm = 4, actions = [Left(), Right()], step = step)
end	

# ╔═╡ 980af3e7-2f1c-49be-8f6b-fc61271dff52
function run_corridor_episode(π; cor = make_corridor())
	s = first(cor.states)
	G = 0.0
	state_history = [s]
	rewardhistory = Vector{Float64}()
	select_action(vec) = sample(eachindex(vec), pweights(vec))
	while s != cor.sterm
		a = cor.actions[select_action(π(s))]
		(s, r) = cor.step(s, a)
		push!(state_history, s)
		push!(rewardhistory, r)
	end
	return state_history, rewardhistory
end

# ╔═╡ edb145d7-95e0-44c9-a60f-57d517edb0c7
run_corridor_episode(s -> [0.5, 0.5]) #this policy chooses randomly between both actions

# ╔═╡ 9d815d9c-6e5a-473e-a395-6f92d504dbf3
md"""
>*Exercise 13.1* Use your knowledge of the gridworld and its dynamics to determine an *exact* symbolic expression for the optimal probability of selecting the right action in Example 13.1

Example 13.1 is a gridworld with 3 non-terminal states and a terminal state at the far right.  The reward is -1 per step.  States 1 and 3 have actions left/right that move in the expected directions but state 2 reverses the directions.  We use a performance measure $J(\mathbf{\theta}) = v_{\pi_\theta}(S)$.  Given our feature representations of $\mathbf{x}(s, \text{right}) = [1, 0]^{\top}$ and $\mathbf{x}(s, \text{left}) = [0, 1]^{\top}$, we can only learn policies that are stochastic in terms of left/right action selection but do not vary between states.  Also observe that due to probability constraints $p_{\text{right}} = 1 - p_{\text{left}}$. For simplicity, we will use the notation $p \dot = p_{\text{left}}$.

$\begin{flalign}
v(S_1) &= p \times v(S_1) + (1-p) \times v(S_2) - 1 \tag{1} \\
v(S_2) &= p \times v(S_3) +(1-p) \times v(S_1) - 1 \tag{2} \\
v(S_3) &= p\times v(S_2) - 1 \tag{3}\\
v(S_2) &= p \times [p\times v(S_2) - 1] +(1-p) \times v(S_1) - 1  \tag{substituting 3 into 2} \\
v(S_2) &= \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \tag{collecting terms} \\
v(S_1) &= p \times v(S_1) + (1-p) \times \left [ \frac{(1-p) \times v(S_1) - (1+p)}{(1+p)(1-p)} \right ] - 1 \tag{substituting above into 1} \\
v(S_1) &= p \times v(S_1) + \left [ \frac{(1-p) \times v(S_1) - (1+p)}{1+p} \right ] - 1 \tag{simplifying} \\
2 &= v(S_1) (-1 + p + \frac{1-p}{1+p}) \tag{collecting v terms} \\
2 &= v(S_1) \frac{(p - 1)(1+p) + 1 - p}{1+p} \tag{combining fractions} \\
2 &= v(S_1) \frac{p^2 - p}{1+p} \tag{simpifying} \\
v(S_1) &= \frac{2(1+p)}{p^2 - p} \\
v(S_2) &= \frac{(1-p) \times [\frac{2(1+p)}{p^2 - p} ] - (1+p)}{(1+p)(1-p)} =  \frac{-\frac{2(1+p)}{p} - (1+p)}{(1+p)(1-p)} = \frac{-2(1+p) - p(1+p)}{p(1+p)(1-p)}\\
&=-\frac{2 + 3p + p^2}{p(1+p)(1-p)} = \frac{(p+2)(p+1)}{p(1+p)(p - 1)} = \frac{p+2}{p(p - 1)} \\
v(S_3) &= p\times \frac{p+2}{p(p - 1)} - 1 = \frac{p+2}{p - 1} - \frac{p-1}{p-1} = \frac{3}{p-1}\\
\end{flalign}$
In order to find the p that maximizes the expected value for state 1, we should differentiate by p and set the result to 0

$\begin{flalign}
0 &= \frac{p(p - 1) - (1+p)(2p - 1)}{(p(p-1))^2} \\
(1+p)(2p - 1) &= p(p - 1) \\
2p - 1 + 2p^2 - p &= p^2 - p \\
2p - 1 + p^2 &= 0 \\
\end{flalign}$

Using the quadratic equation, there are two solutions but since we know p has to be positive we only take that one.

$p = \frac{-2 \pm \sqrt{4 + 4}}{2} = -1 \pm \sqrt{2} \implies p = \sqrt{2} - 1 \approx 0.414$

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.414$ and $p_{\text{right}} \approx 0.586$.  That also implies that $v(S_1) = \frac{2(1+p)}{p^2 - p} = \frac{2\sqrt{2}}{2 - 2 \sqrt{2} + 1 + 1 - \sqrt{2}} = \frac{2\sqrt{2}}{4 - 3\sqrt{2}} = \frac{2\sqrt{2}(4 + 3\sqrt{2})}{16 - 18}=\frac{8\sqrt{2}+12}{-2} = -6-4\sqrt{2}\approx-11.657$

If we solve the same problem at state 2 we get:

$\begin{flalign}
0 &= \frac{p^2 + 4p - 2}{(p-1)^2p^2} \\
0 &= p^2 + 4p - 2 \\
\end{flalign}$

Using the quadratic equation and keeping only the positive solution gives:
$p = \frac{-4 + \sqrt{16 + 8}}{2} = \frac{-4 + 2\sqrt{6}}{2} = \sqrt{6} - 2 \approx 0.4495$.

So, in order to maximize the value at state 1, we have $p_{\text{left}} \approx 0.4495$ and $p_{\text{right}} \approx 0.55$. Which is different from the value we got for state 1.  So There is a different optimal policy depending on the starting state.  It should be obvious for example that starting in the third state results in an optimial policy of choosing the right action every time.  The value functions for each state are plotted below.  The behavior of V(S3) is not well defined at $p=0$ because for any finite V(S2) it should be 0 but the limit approaching from the right side is -3.  This is because for $p=0$ both V(S1) and V(S2) are not finite and the episode never terminates.   
"""

# ╔═╡ e5faaa1b-88cb-43e2-8d04-8972b58b4bda
begin
	v1(p) = (2*(1+p))/(p^2 - p)
	v2(p) = (p+2)/(p*(p - 1))
	v3(p) = 3/(p-1)
	plist = 0.:0.001:1.
	traces = [scatter(x = plist, y = f.(plist), name = n) for (f, n) in zip([v1, v2, v3], ["V(S1)", "V(S2)", "V(S3)"])]
	plot(traces, Layout(yaxis_range = [-100, 0], xaxis_title = "probability of right action", yaxis_title = "State Value"))
end

# ╔═╡ 406638af-1e08-44d2-9ee4-97aa9294a94b
md"""
# 13.2 The Policy Gradient Theorem
"""

# ╔═╡ aa450da4-fe84-4eea-b6c4-9820b7982437
md"""
With continuous policy parametrization, we can smoothly very action selection probabilities by arbitrarily small amounts, something that was not possible with ϵ-greedy action selection.  Therefore stronger convergence guarantees are possible for policy-gradient methods than for action-value methods.

In the episodic case, assuming some particular non-random starting state $s_0$, we define the performance of a policy parametrized by *θ* as:

$\begin{align}
J(\mathbf{\theta}) \hspace{5px} \dot = \hspace{5px} v_{\pi_\mathbf{\theta}}(s_0) \tag{13.4}
\end{align}$

where $v_{\pi_\mathbf{\theta}}$ is the true value function for $\pi_\mathbf{\theta}$, the policy determined by $\mathbf{\theta}$.

The *policy gradient theorem* provides an analytic expression for the gradient of performance with respect to the policy parameter that does *not* involve the derivative of the state distribution:

$\begin{align}
\nabla J(\mathbf{\theta}) \propto \sum_s \mu (s) \sum_a q_\pi (s, a) \nabla \pi (a|s,\mathbf{\theta}) \tag{13.5}
\end{align}$

where the gradients are column vectors of partial derivatives with respect to the components of $\mathbf{\theta}$.  In the episodic case, the constant of proportionality is the average length of an episode, and in the continuing case it is 1.  The distribution here $\mu$ is the on-policy distribution under $\pi$.
"""


# ╔═╡ f924eb30-d1cc-4941-8fb5-ff70ad425ab9
md"""
# 13.3 REINFORCE: Monte Carlo Policy Gradient

If we replace the true action-value function in (13.5) with a learned approximation $\hat q_\pi$, then we have a method called the *all-actions* method because the update involves the sum over all actions.  For the REINFORCE algorithm, we instead sample this value using the actual return and the policy distribution.

We can re-write (13.5) using an expected value under the policy and continue from there:

$\begin{flalign}
\nabla J(\mathbf{\theta}) & \propto \mathbb{E}_\pi \left [ \sum_a q_\pi (S_t, a) \nabla \pi(a|S_t, \mathbf{\theta}) \right ] \tag{13.6}\\
 &= \mathbb{E}_\pi \left [ \sum_a \pi(a|S_t, \mathbf{\theta}) q_\pi (S_t, a) \frac{\nabla \pi(a|S_t, \mathbf{\theta})}{\pi(a|S_t, \mathbf{\theta})} \right ] \tag{multiply and divide by policy} \\
 &= \mathbb{E}_\pi \left [ q_\pi (S_t, A_t) \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace a with sample under policy} \\
 &= \mathbb{E}_\pi \left [ G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})} \right ] \tag{replace value with sample return} \\
\end{flalign}$

Using the expression in the brackets we can write down an update rule for the parameters that can be sampled on each time step.  This is the **REINFORCE update**:

$\begin{align}
\mathbf{\theta}_{t+1} \hspace{5px} \dot = \hspace{5px} \mathbf{\theta}_t + \alpha G_t \frac{\nabla \pi(A_t|S_t, \mathbf{\theta}_t)}{\pi(A_t|S_t, \mathbf{\theta}_t)} \tag{13.8}
\end{align}$

Because it uses all future returns after step t, REINFORCE is a Monte Carlo algorithm and is well defined only for the episodic case.  For implementation purposes we can replace $\frac{\nabla \pi(A_t|S_t, \mathbf{\theta})}{\pi(A_t|S_t, \mathbf{\theta})}$ with $\nabla \ln \pi(A_t|S_t, \mathbf{\theta}_t)$ which is usually refered to as the *eligibility vector*.
"""

# ╔═╡ 2b11ef08-288f-4110-b741-ba580782b6a7
function reinforce_monte_carlo_control(π::Function, ∇lnπ::Function, d::Int64, s0, α, step, sterm, actions; γ = 1.0, max_episodes = 1000, θ = zeros(d), maxsteps = Inf, baseline = 0.0)
	rewards = zeros(max_episodes)
	select_action(vec) = sample(eachindex(vec), pweights(vec))
	
	function run_episode(maxsteps)
		state_history = [s0]
		a = select_action(π(s0, θ))
		action_history = [a]
		(s, r) = step(s0, actions[a])
		reward_history = [r]
		while s != sterm && length(state_history) < maxsteps
			a = select_action(π(s, θ))
			push!(action_history, a)
			(s, r) = step(s, actions[a])
			push!(reward_history, r)
			push!(state_history, s)
		end
		return state_history, action_history, reward_history
	end	
	for i in eachindex(rewards)
		state_history, action_history, reward_history = run_episode(maxsteps)
		G = 0
		#iterate through episode beginning at the end
		for i in reverse(eachindex(reward_history))
			G = (γ * G) + reward_history[i]
			θ .+= α * γ^(i-1) * (G - baseline) .* ∇lnπ(action_history[i], state_history[i], θ)
		end
		rewards[i] = sum(reward_history)
	end
	return rewards, θ
end

# ╔═╡ 71973c41-5fbb-40bf-8cc9-e063c7372a1c
#calculate the softmax of a vector and store the result in another vector of equal length
function soft_max!(v::AbstractVector, out::AbstractVector)
	out .= exp.(v)
	s = sum(out)
	out .= out ./ s
end

# ╔═╡ 49a1d508-b491-4d3a-8415-f5def06884e9
soft_max(v::AbstractVector) = soft_max!(v, similar(v))

# ╔═╡ c6b61679-8a06-47ae-abab-6997ad5cbfea
md"""
Using one hot encoding feature vectors each parameter θ simply represents each state preference, so the policy just takes a softmax of the feature vector to get the action distribution.

$\pi(s, \theta) = \sigma(\mathbf{\theta}) = \frac{e^{\theta_i}}{\sum_{j} e^{\theta_j}} \forall i$

The components of the gradient of the softmax function σ is given by:

$\frac{\partial \sigma(\theta)_i}{\partial \theta_j} = \sigma(\theta)_i(\delta_{ij} - \sigma(\theta)_j)$ 

We can use this expression to get the gradient of the policy output with respect to the parameters:

$\nabla\pi(a| s, \theta) = \sigma(\theta)_a(\delta_{a
j} - \sigma(\theta)_j)$ where we overload the notation for a to also be the index of the selected action

Combining these the *eligibility vector* for one hot encoding and action a is:
$\frac{\nabla \pi(a|s, \mathbf{\theta})}{\pi(a|s, \mathbf{\theta})} = (\delta_{aj} - \sigma(\theta)_j) \forall j$

To calculate this practically, we can use the columns of a one hot matrix who's dimension is dxd and subtract from that the policy vector for state s.
"""

# ╔═╡ c2d8a622-b8f9-454b-9fd1-dc940280624c
function run_corridor_reinforce(;α = 0.0002, θ_0 = [0.0, 0.0], kwargs...)
	features = [1.0 0.0; 0.0 1.0] #feature vectors of length 2 for each action
	avec = zeros(2) #vector to store action output distribution
	e_vec = zeros(2) #storage for eligibility vector

	corridor = make_corridor()

	#we have one parameter for each action
	d = length(corridor.actions)

	#starting state is always 1
	s0 = 1
	
	#policy does not distinguish between states and updates the distribution vector
	π!(s, θ) = soft_max!(θ, avec)
	function ∇lnπ!(a, s, θ)
		π!(s, θ) #fill avec with the appropriate softmax
		#softmax derivative
		for i in eachindex(e_vec)
			e_vec[i] = features[a, i] - avec[i]
		end
		return e_vec
	end

	reinforce_monte_carlo_control(π!, ∇lnπ!, d, s0, α, corridor.step, corridor.sterm, corridor.actions; θ = copy(θ_0), kwargs...)
end

# ╔═╡ 5f91ce14-c9d4-4818-8955-8e7381b4943b
function average_runs(n; kwargs...) 
	runs = Vector{Any}(undef, n)
	@threads for i in 1:n
		runs[i] = run_corridor_reinforce(;kwargs...)[1]
	end
	reduce(+, runs) ./ n
end

# ╔═╡ f9c0aef8-8975-4596-ace1-964269e57bbb
typeof(scatter(x = 1:10, y = 1:10))

# ╔═╡ a45c1930-ad70-44f4-a6bc-10ccb03f65ab
function figure_13_1(αlist; θ_0 = [2.0, 0.0], nruns = 100, seed = 1234, kwargs...)
	Random.seed!(seed)
	traces = [begin
		v = average_runs(nruns; α = α, θ_0 = θ_0, kwargs...)
		scatter(x = eachindex(v) |> collect, y = v, name = latexstring("α = 2^{$(log2(α))}"))
	end
	for α in αlist]

	baselinetrace = scatter(x = 1:1000, y = fill(-6 - 4*sqrt(2), 1000), name = latexstring("v_{\\text{ideal}}(s_0)"), line_dash = "dash", line_color = "gray")

	plot([traces; baselinetrace], Layout(legend_orientation = "h", xaxis_title = "Episode", yaxis_title = "Total reward on episode ($nruns run average)", title = "REINFORCE on the short-Corridor gridworld",  height = 500))
end

# ╔═╡ 71c8d422-8177-4324-b048-98dd39198fee
#in the source code used to generate this for the book found here: http://incompleteideas.net/book/code/figure_13_1.py-remove the episodes start with poor performace because the parameter vector is initialized to prefer left with 95% probability
figure_13_1(2.0 .^ [-12, -13, -14]; θ_0 = [log(19), 0.0], seed = 43432, maxsteps = 1_000)

# ╔═╡ a206c759-3f6e-4003-8cba-5f6ce6742646
md"""
## Figure 13.1
"""

# ╔═╡ 2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
md"""
> *Exercise 13.3* In Section 13.1 we considered policy parameterizations using the soft-max in action preferences (13.2) with linear action preferences (13.3).  For this parameterization, prove that the eligibility vector is
 
$\begin{flalign}
	\nabla \ln \pi(a|s, \mathbf{\theta}) = \mathbf{x}(s, a) - \sum_b \pi(b|s, \mathbf{\theta}) \mathbf{x}(s, b) \tag{13.9}
\end{flalign}$

> using the definitions and elementary calculus.

$\begin{flalign}
\pi(a|s, \mathbf{\theta}) \hspace{5 px} &\dot = \hspace{5 px} \frac{e^{h(s, a, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{13.2} \\
h(s, a, \mathbf{\theta}) &= \mathbf{\theta}^\top \mathbf{x}(s, a) \tag{13.3}
\end{flalign}$

Working from these definitions we can derive the following:

$\begin{flalign}
\nabla h(s, a, \mathbf{\theta}) &= \mathbf{x}(s, a) \tag{by linearity of h}\\
\ln \pi(a|s, \mathbf{\theta}) &= h(s, a, \mathbf{\theta}) - \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \\
\nabla \ln \pi(a|s, \mathbf{\theta}) &= \nabla h(s, a, \mathbf{\theta}) - \nabla \ln{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{distributing gradient} \\
&= \mathbf{x}(s, a) - \frac{\sum_b \nabla  e^{h(s, b, \mathbf{\theta})}}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \frac{\sum_b e^{h(s, b, \mathbf{\theta})} \mathbf{x}(s, b)}{\sum_b e^{h(s, b, \mathbf{\theta})}} \tag{using chain rule} \\
&= \mathbf{x}(s, a) - \sum_i \frac{e^{h(s, i, \mathbf{\theta})} \mathbf{x}(s, i)}{\sum_t e^{h(s, t, \mathbf{\theta})}} \forall i \tag{separating fractions} \\
&= \mathbf{x}(s, a) - \sum_i \pi(i|s, \mathbf{\theta}) \mathbf{x}(s, i) \tag{definition of π} \\
\square\\
\end{flalign}$
"""

# ╔═╡ cc45091e-b889-4d5a-9eef-84d80f792046
md"""
# 13.4 REINFORCE with Baseline
"""

# ╔═╡ 0ab70fc3-6188-42eb-aba2-d808f319be9f
md"""
# Dependencies and Settings
"""

# ╔═╡ ea8cdebd-7a25-49ae-9695-48dda2a880b4
TableOfContents()

# ╔═╡ 00000000-0000-0000-0000-000000000001
PLUTO_PROJECT_TOML_CONTENTS = """
[deps]
Distributions = "31c24e10-a181-5473-b8eb-7969acd0382f"
LaTeXStrings = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
LinearAlgebra = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"
PlutoPlotly = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
PlutoUI = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
Random = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"
StatsBase = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"

[compat]
Distributions = "~0.25.87"
LaTeXStrings = "~1.3.0"
PlutoPlotly = "~0.3.6"
PlutoUI = "~0.7.50"
StatsBase = "~0.33.21"
"""

# ╔═╡ 00000000-0000-0000-0000-000000000002
PLUTO_MANIFEST_TOML_CONTENTS = """
# This file is machine-generated - editing it directly is not advised

julia_version = "1.9.0-rc2"
manifest_format = "2.0"
project_hash = "9ccd09c53a721dc19cf7597969947ce533dde97d"

[[deps.AbstractPlutoDingetjes]]
deps = ["Pkg"]
git-tree-sha1 = "8eaf9f1b4921132a4cff3f36a1d9ba923b14a481"
uuid = "6e696c72-6542-2067-7265-42206c756150"
version = "1.1.4"

[[deps.ArgTools]]
uuid = "0dad84c5-d112-42e6-8d28-ef12dabb789f"
version = "1.1.1"

[[deps.Artifacts]]
uuid = "56f22d72-fd6d-98f1-02f0-08ddc0907c33"

[[deps.Base64]]
uuid = "2a0f44e3-6c83-55bd-87e4-b1978d98bd5f"

[[deps.Calculus]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "f641eb0a4f00c343bbc32346e1217b86f3ce9dad"
uuid = "49dc2e85-a5d0-5ad3-a950-438e2897f1b9"
version = "0.5.1"

[[deps.ColorSchemes]]
deps = ["ColorTypes", "ColorVectorSpace", "Colors", "FixedPointNumbers", "Random", "SnoopPrecompile"]
git-tree-sha1 = "aa3edc8f8dea6cbfa176ee12f7c2fc82f0608ed3"
uuid = "35d6a980-a343-548e-a6ea-1d62b119f2f4"
version = "3.20.0"

[[deps.ColorTypes]]
deps = ["FixedPointNumbers", "Random"]
git-tree-sha1 = "eb7f0f8307f71fac7c606984ea5fb2817275d6e4"
uuid = "3da002f7-5984-5a60-b8a6-cbb66c0b333f"
version = "0.11.4"

[[deps.ColorVectorSpace]]
deps = ["ColorTypes", "FixedPointNumbers", "LinearAlgebra", "SpecialFunctions", "Statistics", "TensorCore"]
git-tree-sha1 = "600cc5508d66b78aae350f7accdb58763ac18589"
uuid = "c3611d14-8923-5661-9e6a-0046d554d3a4"
version = "0.9.10"

[[deps.Colors]]
deps = ["ColorTypes", "FixedPointNumbers", "Reexport"]
git-tree-sha1 = "fc08e5930ee9a4e03f84bfb5211cb54e7769758a"
uuid = "5ae59095-9a9b-59fe-a467-6f913c188581"
version = "0.12.10"

[[deps.Compat]]
deps = ["UUIDs"]
git-tree-sha1 = "7a60c856b9fa189eb34f5f8a6f6b5529b7942957"
uuid = "34da2185-b29b-5c13-b0c7-acf172513d20"
version = "4.6.1"
weakdeps = ["Dates", "LinearAlgebra"]

    [deps.Compat.extensions]
    CompatLinearAlgebraExt = "LinearAlgebra"

[[deps.CompilerSupportLibraries_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "e66e0078-7015-5450-92f7-15fbd957f2ae"
version = "1.0.2+0"

[[deps.DataAPI]]
git-tree-sha1 = "e8119c1a33d267e16108be441a287a6981ba1630"
uuid = "9a962f9c-6df0-11e9-0e5d-c546b8b5ee8a"
version = "1.14.0"

[[deps.DataStructures]]
deps = ["Compat", "InteractiveUtils", "OrderedCollections"]
git-tree-sha1 = "d1fff3a548102f48987a52a2e0d114fa97d730f0"
uuid = "864edb3b-99cc-5e75-8d2d-829cb0a9cfe8"
version = "0.18.13"

[[deps.Dates]]
deps = ["Printf"]
uuid = "ade2ca70-3891-5945-98fb-dc099432e06a"

[[deps.DelimitedFiles]]
deps = ["Mmap"]
git-tree-sha1 = "9e2f36d3c96a820c678f2f1f1782582fcf685bae"
uuid = "8bb1440f-4735-579b-a4ab-409b98df4dab"
version = "1.9.1"

[[deps.Distributions]]
deps = ["FillArrays", "LinearAlgebra", "PDMats", "Printf", "QuadGK", "Random", "SparseArrays", "SpecialFunctions", "Statistics", "StatsBase", "StatsFuns", "Test"]
git-tree-sha1 = "13027f188d26206b9e7b863036f87d2f2e7d013a"
uuid = "31c24e10-a181-5473-b8eb-7969acd0382f"
version = "0.25.87"

    [deps.Distributions.extensions]
    DistributionsChainRulesCoreExt = "ChainRulesCore"
    DistributionsDensityInterfaceExt = "DensityInterface"

    [deps.Distributions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    DensityInterface = "b429d917-457f-4dbc-8f4c-0cc954292b1d"

[[deps.DocStringExtensions]]
deps = ["LibGit2"]
git-tree-sha1 = "2fb1e02f2b635d0845df5d7c167fec4dd739b00d"
uuid = "ffbed154-4ef7-542d-bbb7-c09d3a79fcae"
version = "0.9.3"

[[deps.Downloads]]
deps = ["ArgTools", "FileWatching", "LibCURL", "NetworkOptions"]
uuid = "f43a241f-c20a-4ad4-852c-f6b1247861c6"
version = "1.6.0"

[[deps.DualNumbers]]
deps = ["Calculus", "NaNMath", "SpecialFunctions"]
git-tree-sha1 = "5837a837389fccf076445fce071c8ddaea35a566"
uuid = "fa6b7ba4-c1ee-5f82-b5fc-ecf0adba8f74"
version = "0.6.8"

[[deps.FileWatching]]
uuid = "7b1f6079-737a-58dc-b8bc-7a2ca5c1b5ee"

[[deps.FillArrays]]
deps = ["LinearAlgebra", "Random", "SparseArrays", "Statistics"]
git-tree-sha1 = "fc86b4fd3eff76c3ce4f5e96e2fdfa6282722885"
uuid = "1a297f60-69ca-5386-bcde-b61e274b549b"
version = "1.0.0"

[[deps.FixedPointNumbers]]
deps = ["Statistics"]
git-tree-sha1 = "335bfdceacc84c5cdf16aadc768aa5ddfc5383cc"
uuid = "53c48c17-4a7d-5ca2-90c5-79b7896eea93"
version = "0.8.4"

[[deps.HypergeometricFunctions]]
deps = ["DualNumbers", "LinearAlgebra", "OpenLibm_jll", "SpecialFunctions"]
git-tree-sha1 = "432b5b03176f8182bd6841fbfc42c718506a2d5f"
uuid = "34004b35-14d8-5ef3-9330-4cdb6864b03a"
version = "0.3.15"

[[deps.Hyperscript]]
deps = ["Test"]
git-tree-sha1 = "8d511d5b81240fc8e6802386302675bdf47737b9"
uuid = "47d2ed2b-36de-50cf-bf87-49c2cf4b8b91"
version = "0.0.4"

[[deps.HypertextLiteral]]
deps = ["Tricks"]
git-tree-sha1 = "c47c5fa4c5308f27ccaac35504858d8914e102f9"
uuid = "ac1192a8-f4b3-4bfe-ba22-af5b92cd3ab2"
version = "0.9.4"

[[deps.IOCapture]]
deps = ["Logging", "Random"]
git-tree-sha1 = "f7be53659ab06ddc986428d3a9dcc95f6fa6705a"
uuid = "b5f81e59-6552-4d32-b1f0-c071b021bf89"
version = "0.2.2"

[[deps.InteractiveUtils]]
deps = ["Markdown"]
uuid = "b77e0a4c-d291-57a0-90e8-8db25a27a240"

[[deps.IrrationalConstants]]
git-tree-sha1 = "630b497eafcc20001bba38a4651b327dcfc491d2"
uuid = "92d709cd-6900-40b7-9082-c6be49f344b6"
version = "0.2.2"

[[deps.JLLWrappers]]
deps = ["Preferences"]
git-tree-sha1 = "abc9885a7ca2052a736a600f7fa66209f96506e1"
uuid = "692b3bcd-3c85-4b1f-b108-f13ce0eb3210"
version = "1.4.1"

[[deps.JSON]]
deps = ["Dates", "Mmap", "Parsers", "Unicode"]
git-tree-sha1 = "3c837543ddb02250ef42f4738347454f95079d4e"
uuid = "682c06a0-de6a-54ab-a142-c8b1cf79cde6"
version = "0.21.3"

[[deps.LaTeXStrings]]
git-tree-sha1 = "f2355693d6778a178ade15952b7ac47a4ff97996"
uuid = "b964fa9f-0449-5b57-a5c2-d3ea65f4040f"
version = "1.3.0"

[[deps.LibCURL]]
deps = ["LibCURL_jll", "MozillaCACerts_jll"]
uuid = "b27032c2-a3e7-50c8-80cd-2d36dbcbfd21"
version = "0.6.3"

[[deps.LibCURL_jll]]
deps = ["Artifacts", "LibSSH2_jll", "Libdl", "MbedTLS_jll", "Zlib_jll", "nghttp2_jll"]
uuid = "deac9b47-8bc7-5906-a0fe-35ac56dc84c0"
version = "7.84.0+0"

[[deps.LibGit2]]
deps = ["Base64", "NetworkOptions", "Printf", "SHA"]
uuid = "76f85450-5226-5b5a-8eaa-529ad045b433"

[[deps.LibSSH2_jll]]
deps = ["Artifacts", "Libdl", "MbedTLS_jll"]
uuid = "29816b5a-b9ab-546f-933c-edad1886dfa8"
version = "1.10.2+0"

[[deps.Libdl]]
uuid = "8f399da3-3557-5675-b5ff-fb832c97cbdb"

[[deps.LinearAlgebra]]
deps = ["Libdl", "OpenBLAS_jll", "libblastrampoline_jll"]
uuid = "37e2e46d-f89d-539d-b4ee-838fcccc9c8e"

[[deps.LogExpFunctions]]
deps = ["DocStringExtensions", "IrrationalConstants", "LinearAlgebra"]
git-tree-sha1 = "0a1b7c2863e44523180fdb3146534e265a91870b"
uuid = "2ab3a3ac-af41-5b50-aa03-7779005ae688"
version = "0.3.23"

    [deps.LogExpFunctions.extensions]
    LogExpFunctionsChainRulesCoreExt = "ChainRulesCore"
    LogExpFunctionsChangesOfVariablesExt = "ChangesOfVariables"
    LogExpFunctionsInverseFunctionsExt = "InverseFunctions"

    [deps.LogExpFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    ChangesOfVariables = "9e997f8a-9a97-42d5-a9f1-ce6bfc15e2c0"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.Logging]]
uuid = "56ddb016-857b-54e1-b83d-db4d58db5568"

[[deps.MIMEs]]
git-tree-sha1 = "65f28ad4b594aebe22157d6fac869786a255b7eb"
uuid = "6c6e2e6c-3030-632d-7369-2d6c69616d65"
version = "0.1.4"

[[deps.Markdown]]
deps = ["Base64"]
uuid = "d6f4376e-aef5-505a-96c1-9c027394607a"

[[deps.MbedTLS_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "c8ffd9c3-330d-5841-b78e-0817d7145fa1"
version = "2.28.2+0"

[[deps.Missings]]
deps = ["DataAPI"]
git-tree-sha1 = "f66bdc5de519e8f8ae43bdc598782d35a25b1272"
uuid = "e1d29d7a-bbdc-5cf2-9ac0-f12de2c33e28"
version = "1.1.0"

[[deps.Mmap]]
uuid = "a63ad114-7e13-5084-954f-fe012c677804"

[[deps.MozillaCACerts_jll]]
uuid = "14a3606d-f60d-562e-9121-12d972cd8159"
version = "2022.10.11"

[[deps.NaNMath]]
deps = ["OpenLibm_jll"]
git-tree-sha1 = "0877504529a3e5c3343c6f8b4c0381e57e4387e4"
uuid = "77ba4419-2d1f-58cd-9bb1-8ffee604a2e3"
version = "1.0.2"

[[deps.NetworkOptions]]
uuid = "ca575930-c2e3-43a9-ace4-1e988b2c1908"
version = "1.2.0"

[[deps.OpenBLAS_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "Libdl"]
uuid = "4536629a-c528-5b80-bd46-f80d51c5b363"
version = "0.3.21+4"

[[deps.OpenLibm_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "05823500-19ac-5b8b-9628-191a04bc5112"
version = "0.8.1+0"

[[deps.OpenSpecFun_jll]]
deps = ["Artifacts", "CompilerSupportLibraries_jll", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "13652491f6856acfd2db29360e1bbcd4565d04f1"
uuid = "efe28fd5-8261-553b-a9e1-b2916fc3738e"
version = "0.5.5+0"

[[deps.OrderedCollections]]
git-tree-sha1 = "d321bf2de576bf25ec4d3e4360faca399afca282"
uuid = "bac558e1-5e72-5ebc-8fee-abe8a469f55d"
version = "1.6.0"

[[deps.PDMats]]
deps = ["LinearAlgebra", "SparseArrays", "SuiteSparse"]
git-tree-sha1 = "67eae2738d63117a196f497d7db789821bce61d1"
uuid = "90014a1f-27ba-587c-ab20-58faa44d9150"
version = "0.11.17"

[[deps.Parameters]]
deps = ["OrderedCollections", "UnPack"]
git-tree-sha1 = "34c0e9ad262e5f7fc75b10a9952ca7692cfc5fbe"
uuid = "d96e819e-fc66-5662-9728-84c9c7592b0a"
version = "0.12.3"

[[deps.Parsers]]
deps = ["Dates", "SnoopPrecompile"]
git-tree-sha1 = "478ac6c952fddd4399e71d4779797c538d0ff2bf"
uuid = "69de0a69-1ddd-5017-9359-2bf0b02dc9f0"
version = "2.5.8"

[[deps.Pkg]]
deps = ["Artifacts", "Dates", "Downloads", "FileWatching", "LibGit2", "Libdl", "Logging", "Markdown", "Printf", "REPL", "Random", "SHA", "Serialization", "TOML", "Tar", "UUIDs", "p7zip_jll"]
uuid = "44cfe95a-1eb2-52ea-b672-e2afdf69b78f"
version = "1.9.0"

[[deps.PlotlyBase]]
deps = ["ColorSchemes", "Dates", "DelimitedFiles", "DocStringExtensions", "JSON", "LaTeXStrings", "Logging", "Parameters", "Pkg", "REPL", "Requires", "Statistics", "UUIDs"]
git-tree-sha1 = "56baf69781fc5e61607c3e46227ab17f7040ffa2"
uuid = "a03496cd-edff-5a9b-9e67-9cda94a718b5"
version = "0.8.19"

[[deps.PlutoPlotly]]
deps = ["AbstractPlutoDingetjes", "Colors", "Dates", "HypertextLiteral", "InteractiveUtils", "LaTeXStrings", "Markdown", "PlotlyBase", "PlutoUI", "Reexport"]
git-tree-sha1 = "dec81dcd52748ffc59ce3582e709414ff78d947f"
uuid = "8e989ff0-3d88-8e9f-f020-2b208a939ff0"
version = "0.3.6"

[[deps.PlutoUI]]
deps = ["AbstractPlutoDingetjes", "Base64", "ColorTypes", "Dates", "FixedPointNumbers", "Hyperscript", "HypertextLiteral", "IOCapture", "InteractiveUtils", "JSON", "Logging", "MIMEs", "Markdown", "Random", "Reexport", "URIs", "UUIDs"]
git-tree-sha1 = "5bb5129fdd62a2bbbe17c2756932259acf467386"
uuid = "7f904dfe-b85e-4ff6-b463-dae2292396a8"
version = "0.7.50"

[[deps.Preferences]]
deps = ["TOML"]
git-tree-sha1 = "47e5f437cc0e7ef2ce8406ce1e7e24d44915f88d"
uuid = "21216c6a-2e73-6563-6e65-726566657250"
version = "1.3.0"

[[deps.Printf]]
deps = ["Unicode"]
uuid = "de0858da-6303-5e67-8744-51eddeeeb8d7"

[[deps.QuadGK]]
deps = ["DataStructures", "LinearAlgebra"]
git-tree-sha1 = "6ec7ac8412e83d57e313393220879ede1740f9ee"
uuid = "1fd47b50-473d-5c70-9696-f719f8f3bcdc"
version = "2.8.2"

[[deps.REPL]]
deps = ["InteractiveUtils", "Markdown", "Sockets", "Unicode"]
uuid = "3fa0cd96-eef1-5676-8a61-b3b8758bbffb"

[[deps.Random]]
deps = ["SHA", "Serialization"]
uuid = "9a3f8284-a2c9-5f02-9a11-845980a1fd5c"

[[deps.Reexport]]
git-tree-sha1 = "45e428421666073eab6f2da5c9d310d99bb12f9b"
uuid = "189a3867-3050-52da-a836-e630ba90ab69"
version = "1.2.2"

[[deps.Requires]]
deps = ["UUIDs"]
git-tree-sha1 = "838a3a4188e2ded87a4f9f184b4b0d78a1e91cb7"
uuid = "ae029012-a4dd-5104-9daa-d747884805df"
version = "1.3.0"

[[deps.Rmath]]
deps = ["Random", "Rmath_jll"]
git-tree-sha1 = "f65dcb5fa46aee0cf9ed6274ccbd597adc49aa7b"
uuid = "79098fc4-a85e-5d69-aa6a-4863f24498fa"
version = "0.7.1"

[[deps.Rmath_jll]]
deps = ["Artifacts", "JLLWrappers", "Libdl", "Pkg"]
git-tree-sha1 = "6ed52fdd3382cf21947b15e8870ac0ddbff736da"
uuid = "f50d1b31-88e8-58de-be2c-1cc44531875f"
version = "0.4.0+0"

[[deps.SHA]]
uuid = "ea8e919c-243c-51af-8825-aaa63cd721ce"
version = "0.7.0"

[[deps.Serialization]]
uuid = "9e88b42a-f829-5b0c-bbe9-9e923198166b"

[[deps.SnoopPrecompile]]
deps = ["Preferences"]
git-tree-sha1 = "e760a70afdcd461cf01a575947738d359234665c"
uuid = "66db9d55-30c0-4569-8b51-7e840670fc0c"
version = "1.0.3"

[[deps.Sockets]]
uuid = "6462fe0b-24de-5631-8697-dd941f90decc"

[[deps.SortingAlgorithms]]
deps = ["DataStructures"]
git-tree-sha1 = "a4ada03f999bd01b3a25dcaa30b2d929fe537e00"
uuid = "a2af1166-a08f-5f64-846c-94a0d3cef48c"
version = "1.1.0"

[[deps.SparseArrays]]
deps = ["Libdl", "LinearAlgebra", "Random", "Serialization", "SuiteSparse_jll"]
uuid = "2f01184e-e22b-5df5-ae63-d93ebab69eaf"

[[deps.SpecialFunctions]]
deps = ["IrrationalConstants", "LogExpFunctions", "OpenLibm_jll", "OpenSpecFun_jll"]
git-tree-sha1 = "ef28127915f4229c971eb43f3fc075dd3fe91880"
uuid = "276daf66-3868-5448-9aa4-cd146d93841b"
version = "2.2.0"

    [deps.SpecialFunctions.extensions]
    SpecialFunctionsChainRulesCoreExt = "ChainRulesCore"

    [deps.SpecialFunctions.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"

[[deps.Statistics]]
deps = ["LinearAlgebra", "SparseArrays"]
uuid = "10745b16-79ce-11e8-11f9-7d13ad32a3b2"
version = "1.9.0"

[[deps.StatsAPI]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "45a7769a04a3cf80da1c1c7c60caf932e6f4c9f7"
uuid = "82ae8749-77ed-4fe6-ae5f-f523153014b0"
version = "1.6.0"

[[deps.StatsBase]]
deps = ["DataAPI", "DataStructures", "LinearAlgebra", "LogExpFunctions", "Missings", "Printf", "Random", "SortingAlgorithms", "SparseArrays", "Statistics", "StatsAPI"]
git-tree-sha1 = "d1bf48bfcc554a3761a133fe3a9bb01488e06916"
uuid = "2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91"
version = "0.33.21"

[[deps.StatsFuns]]
deps = ["HypergeometricFunctions", "IrrationalConstants", "LogExpFunctions", "Reexport", "Rmath", "SpecialFunctions"]
git-tree-sha1 = "f625d686d5a88bcd2b15cd81f18f98186fdc0c9a"
uuid = "4c63d2b9-4356-54db-8cca-17b64c39e42c"
version = "1.3.0"

    [deps.StatsFuns.extensions]
    StatsFunsChainRulesCoreExt = "ChainRulesCore"
    StatsFunsInverseFunctionsExt = "InverseFunctions"

    [deps.StatsFuns.weakdeps]
    ChainRulesCore = "d360d2e6-b24c-11e9-a2a3-2a2ae2dbcce4"
    InverseFunctions = "3587e190-3f89-42d0-90ee-14403ec27112"

[[deps.SuiteSparse]]
deps = ["Libdl", "LinearAlgebra", "Serialization", "SparseArrays"]
uuid = "4607b0f0-06f3-5cda-b6b1-a6196a1729e9"

[[deps.SuiteSparse_jll]]
deps = ["Artifacts", "Libdl", "Pkg", "libblastrampoline_jll"]
uuid = "bea87d4a-7f5b-5778-9afe-8cc45184846c"
version = "5.10.1+6"

[[deps.TOML]]
deps = ["Dates"]
uuid = "fa267f1f-6049-4f14-aa54-33bafae1ed76"
version = "1.0.3"

[[deps.Tar]]
deps = ["ArgTools", "SHA"]
uuid = "a4e569a6-e804-4fa4-b0f3-eef7a1d5b13e"
version = "1.10.0"

[[deps.TensorCore]]
deps = ["LinearAlgebra"]
git-tree-sha1 = "1feb45f88d133a655e001435632f019a9a1bcdb6"
uuid = "62fd8b95-f654-4bbd-a8a5-9c27f68ccd50"
version = "0.1.1"

[[deps.Test]]
deps = ["InteractiveUtils", "Logging", "Random", "Serialization"]
uuid = "8dfed614-e22c-5e08-85e1-65c5234f0b40"

[[deps.Tricks]]
git-tree-sha1 = "aadb748be58b492045b4f56166b5188aa63ce549"
uuid = "410a4b4d-49e4-4fbc-ab6d-cb71b17b3775"
version = "0.1.7"

[[deps.URIs]]
git-tree-sha1 = "074f993b0ca030848b897beff716d93aca60f06a"
uuid = "5c2747f8-b7ea-4ff2-ba2e-563bfd36b1d4"
version = "1.4.2"

[[deps.UUIDs]]
deps = ["Random", "SHA"]
uuid = "cf7118a7-6976-5b1a-9a39-7adc72f591a4"

[[deps.UnPack]]
git-tree-sha1 = "387c1f73762231e86e0c9c5443ce3b4a0a9a0c2b"
uuid = "3a884ed6-31ef-47d7-9d2a-63182c4928ed"
version = "1.0.2"

[[deps.Unicode]]
uuid = "4ec0a83e-493e-50e2-b9ac-8f72acf5a8f5"

[[deps.Zlib_jll]]
deps = ["Libdl"]
uuid = "83775a58-1f1d-513f-b197-d71354ab007a"
version = "1.2.13+0"

[[deps.libblastrampoline_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850b90-86db-534c-a0d3-1478176c7d93"
version = "5.4.0+0"

[[deps.nghttp2_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "8e850ede-7688-5339-a07c-302acd2aaf8d"
version = "1.48.0+0"

[[deps.p7zip_jll]]
deps = ["Artifacts", "Libdl"]
uuid = "3f19e933-33d8-53b3-aaab-bd5110c3b7a0"
version = "17.4.0+0"
"""

# ╔═╡ Cell order:
# ╟─36a6e43f-6bcf-4c27-bfbb-047760e77ada
# ╟─2501cbc0-9772-4b2f-ab01-ef7903e62950
# ╟─7a6fb1f0-fc3c-4c29-a6d9-769d32ca98a9
# ╠═1b89a5be-d4f6-43b6-b778-0895d77d0962
# ╠═759afa53-2b01-4d9b-b398-80120626634f
# ╠═97046258-7753-4edb-b0c9-0981d587ad35
# ╠═423321cc-1c8c-44a0-bd8e-a4d3cb68962b
# ╠═980af3e7-2f1c-49be-8f6b-fc61271dff52
# ╠═edb145d7-95e0-44c9-a60f-57d517edb0c7
# ╟─9d815d9c-6e5a-473e-a395-6f92d504dbf3
# ╟─e5faaa1b-88cb-43e2-8d04-8972b58b4bda
# ╟─406638af-1e08-44d2-9ee4-97aa9294a94b
# ╟─aa450da4-fe84-4eea-b6c4-9820b7982437
# ╟─f924eb30-d1cc-4941-8fb5-ff70ad425ab9
# ╠═2b11ef08-288f-4110-b741-ba580782b6a7
# ╠═71973c41-5fbb-40bf-8cc9-e063c7372a1c
# ╠═49a1d508-b491-4d3a-8415-f5def06884e9
# ╟─c6b61679-8a06-47ae-abab-6997ad5cbfea
# ╠═c2d8a622-b8f9-454b-9fd1-dc940280624c
# ╠═5f91ce14-c9d4-4818-8955-8e7381b4943b
# ╠═f9c0aef8-8975-4596-ace1-964269e57bbb
# ╠═a45c1930-ad70-44f4-a6bc-10ccb03f65ab
# ╟─71c8d422-8177-4324-b048-98dd39198fee
# ╟─a206c759-3f6e-4003-8cba-5f6ce6742646
# ╟─2e6d0374-1c93-48c8-b8ba-dd1a0c682d01
# ╠═cc45091e-b889-4d5a-9eef-84d80f792046
# ╟─0ab70fc3-6188-42eb-aba2-d808f319be9f
# ╠═d04d4234-d97f-11ed-2ea3-85ee0fc3bd70
# ╠═ea8cdebd-7a25-49ae-9695-48dda2a880b4
# ╟─00000000-0000-0000-0000-000000000001
# ╟─00000000-0000-0000-0000-000000000002
